{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1 - Federico García e Ignacio Rupérez. MBD O1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Madrid. 28th May 2017.** In the following notebook, we have created a model able to classify fake and real news with a **96.06% accuracy**. To do so we have applied different iterative Natural Language Processing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import collections\n",
    "import nltk.classify.util\n",
    "import logging\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from __future__ import print_function\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we import the csv with the labeled (fake or real) pieces of news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import 'fake_or_real_news.csv' \n",
    "df = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "    \n",
    "# Inspect shape of 'df' \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the training set has 3999 pieces of news and 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2   3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3  10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4    875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "4  It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print first lines of 'df' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns are ID, title, text, label, X1 and X2. We can set the ID column to be the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label   X1   X2  \n",
       "ID                                                                        \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "875    It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set index \n",
    "df = df.set_index(\"ID\")\n",
    "\n",
    "# Print first lines of 'df' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X1 and X2 columns seem to be empty, but we should check them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if all values in X1 and X2 are NaN\n",
    "len(df)-sum(pd.isnull(df['X1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)-sum(pd.isnull(df['X2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that 33 rows have some values in column X1, and 2 rows have some values in column X1. Let's sort the dataframe by X1 to see what are those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   title  \\\n",
      "ID                                                         \n",
      "6268   Chart Of The Day: Since 2009—–Recovery For The 5%   \n",
      "10499  30th Infantry Division: “Work Horse of the Wes...   \n",
      "6717                    Jim Rogers: It’s Time To Prepare   \n",
      "8748       WATCH: Mass Shooting Occurs During #TrumpRiot   \n",
      "5741                                       Why Trump Won   \n",
      "10138                Inside The Mind Of An FBI Informant   \n",
      "10492  TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...   \n",
      "6404   #BREAKING: SECOND Assassination Attempt On Tru...   \n",
      "8470   The Amish In America Commit Their Vote To Dona...   \n",
      "7559   STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...   \n",
      "9954   Incredible smoke haze seen outside NDTV office...   \n",
      "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
      "9203         Political Correctness for Yuengling Brewery   \n",
      "9097                    ICE Agent Commits Suicide in NYC   \n",
      "7375   Shallow 5.4 magnitude earthquake rattles centr...   \n",
      "9                   Planned Parenthood’s lobbying effort   \n",
      "3508                          Belgian police mount raids   \n",
      "2738                               Ted Cruz launches bid   \n",
      "3624            Suspects In Paris Magazine Attack Killed   \n",
      "5248         Clinton Cries Racism Tagging Trump with KKK   \n",
      "2943               Islamic State admits defeat in Kobani   \n",
      "356                       Black Hawk crashes off Florida   \n",
      "2786      Afghanistan: 19 die in air attacks on hospital   \n",
      "496                     Nearly 300K New Jobs In February   \n",
      "3622   Al Qaeda rep says group directed Paris magazin...   \n",
      "4748                                          Trump &amp   \n",
      "4953        Gary Johnson Avoids Typical Third-Party Fade   \n",
      "4025   State Dept. IDs 2 Americans killed in Nepal quake   \n",
      "1602   Poll gives Biden edge over Clinton against GOP...   \n",
      "3634       The Latest On Paris Attack: Manhunt Continues   \n",
      "...                                                  ...   \n",
      "9538            Is Global Warming “An Inconvenient Lie”?   \n",
      "5961   Trump's Election Marks the End of Liberal Capi...   \n",
      "9752   Russia suggests joint engineering troops’ dril...   \n",
      "7117   Military Veterans Are Helping To Save Coral Re...   \n",
      "6161         Trump: Israel is a ray of hope to the world   \n",
      "2504   Along the migrant trail, pressure grows to clo...   \n",
      "9466   News: Inspiring: When This Woman Was Feeling T...   \n",
      "8271   Smart Meter Case Testimony Before the Pennsylv...   \n",
      "6596   Reporters Stunned to Learn Trump Fans Lining U...   \n",
      "8170   IOWA FARMER CLAIMS BILL CLINTON HAD SEX WITH C...   \n",
      "3216   Republicans Reject Calls on Guantanamo Bay Clo...   \n",
      "4201   GOP elites are now resigned to Donald Trump as...   \n",
      "4192   Trump's Indiana win raises unsettling question...   \n",
      "9850   Cypriot leaders to continue talks in November: UN   \n",
      "3598   Freed Al Qaeda operative floated as part of pr...   \n",
      "10244        This Viral Video Has Hillary Running Scared   \n",
      "5837   Comment on After Sweeping Election, First Thin...   \n",
      "2787   Obama to keep 5,500 US troops in Afghanistan b...   \n",
      "9229   Family Remembers Queens Sucker Punch Victim ‘H...   \n",
      "8556   Black Americans Going For Donald Trump In Reco...   \n",
      "2966       Success against ISIS requires a team of teams   \n",
      "1433   The Edge: Cruz, Trump get media attention post...   \n",
      "328    NY police investigate possible sighting of esc...   \n",
      "4723   “He and I Haven’t Spoken”: Trump and Pence Are...   \n",
      "7927   FBI Director may be sacked for intrusion into ...   \n",
      "802                   Fault lines: GOP civil war deepens   \n",
      "1026   Clinton, under fire for oil and gas donations,...   \n",
      "1459   His rivals are saying the same thing in differ...   \n",
      "5366   The Genocide of Indigenous Peoples in North Am...   \n",
      "9673                                   Checkmating Obama   \n",
      "\n",
      "                                                    text  \\\n",
      "ID                                                         \n",
      "6268                              Stagnation for the 95%   \n",
      "10499                             The Big Picture TV-211   \n",
      "6717    Economic And Financial Collapse Imminent (VIDEO)   \n",
      "8748                               Media Ignores (Video)   \n",
      "5741                                    Why Clinton Lost   \n",
      "10138          Terri Linnell Admits Role As Gov’t Snitch   \n",
      "10492                    “THE END OF LIFE AS WE KNOW IT”   \n",
      "6404                        Suspect Detained (LIVE BLOG)   \n",
      "8470    Mathematically Guaranteeing Him A Presidentia...   \n",
      "7559        GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS   \n",
      "9954                  bursting of firecrackers suspected   \n",
      "10194                   Leonardo DiCaprio to the rescue?   \n",
      "9203                     What About Our Opioid Epidemic?   \n",
      "9097    Leaves Note Revealing Gov’t Plans to Round-up...   \n",
      "7375                            shakes buildings in Rome   \n",
      "9                         pay raises for federal workers   \n",
      "3508        prosecutors acknowledge missed opportunities   \n",
      "2738           Some pundits paint him as scary extremist   \n",
      "3624              Market Gunman And 4 Hostages Also Dead   \n",
      "5248                               Trump Says 'She Lies'   \n",
      "2943                                   blames airstrikes   \n",
      "356                                  human remains found   \n",
      "2786                                  U.S. investigating   \n",
      "496                     Unemployment Dips To 5.5 Percent   \n",
      "3622                            US issues travel warning   \n",
      "4748    Clinton Were Very Convincing...on How Lousy t...   \n",
      "4953                     Best Polling Since Perot in ‘92   \n",
      "4025                            2 others reportedly dead   \n",
      "1602                                VP meets with Trumka   \n",
      "3634                        Brothers Were On No-Fly List   \n",
      "...                                                  ...   \n",
      "9538   geoengineeringwatch.org \\nGlobal warming disin...   \n",
      "5961   Here's something interesting from The Unz Revi...   \n",
      "9752   Russia suggests joint engineering troops’ dril...   \n",
      "7117   ‹ › Arnaldo Rodgers is a trained and educated ...   \n",
      "6161   November 11, 2016 Trump: Israel is a ray of ho...   \n",
      "2504   With Slovenia behind them and Austria just ahe...   \n",
      "9466   Email \\nWell, if this doesn’t inspire you to d...   \n",
      "8271   By Catherine J. Frompovich \\nThis is the conti...   \n",
      "6596   Hillary Camp Caught on Camera Telling Tiny Cro...   \n",
      "8170   Email \\n\\nSioux Falls, IA | During his 1992 pr...   \n",
      "3216   Senate Republicans are rejecting renewed calls...   \n",
      "4201   Throughout the Republican Party, from New Hamp...   \n",
      "4192   Donald Trump became the presumptive Republican...   \n",
      "9850   EU UN Secretary-General Ban Ki-moon (C) meets ...   \n",
      "3598   An admitted Al Qaeda agent released this month...   \n",
      "10244  This Viral Video Has Hillary Running Scared Th...   \n",
      "5837   Dispatches from Wolf Country –Sitting in my ca...   \n",
      "2787   President Obama announced Thursday he will kee...   \n",
      "9229   Family Remembers Queens Sucker Punch Victim ‘H...   \n",
      "8556   NTEB Ads Privacy Policy Black Americans Going ...   \n",
      "2966   A truck bomb detonates in a lively street, con...   \n",
      "1433   **Want FOX News First in your inbox every day?...   \n",
      "328    New York State Police are looking into a possi...   \n",
      "4723   In the middle of a heated exchange about the S...   \n",
      "7927   FBI Director may be sacked for intrusion into ...   \n",
      "802    Washington (CNN) Donald Trump is poised to bre...   \n",
      "1026   Clinton lost her temper at an event on Thursda...   \n",
      "1459   Chris Christie depicts Marco Rubio as a truant...   \n",
      "5366   Prof. Tony Hall Speaks Out on Mohawk Territory...   \n",
      "9673   Originally published by the Jerusalem Post . \\...   \n",
      "\n",
      "                                                   label  \\\n",
      "ID                                                         \n",
      "6268    Chart Of The Day: Since 2009 Recovery For The 5%   \n",
      "10499  Published on Oct 27, 2016 by Jeff Quitney The ...   \n",
      "6717   By: The Voice of Reason | Regardless of how mu...   \n",
      "8748     WATCH: Mass Shooting Occurs During #TrumpRio...   \n",
      "5741     WashingtonsBlog \\nBy Robert Parry, the inves...   \n",
      "10138  Inside The Mind Of An FBI Informant; Terri Lin...   \n",
      "10492  Paul Joseph Watson Senior British army officer...   \n",
      "6404   We Are Change \\nDonald Trump on Saturday was q...   \n",
      "8470   18 SHARE The Amish in America have committed t...   \n",
      "7559   Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...   \n",
      "9954   Incredible smoke haze seen outside NDTV office...   \n",
      "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
      "9203   We Are Change \\n\\nIn today’s political climate...   \n",
      "9097   Email Print After writing a lengthy suicide no...   \n",
      "7375    00 UTC © USGS Map of the earthquake's epicent...   \n",
      "9                               and the future Fed rates   \n",
      "3508   Belgian authorities missed a chance to press a...   \n",
      "2738   Before he got to repealing ObamaCare, before h...   \n",
      "3624   Suspects In Paris Magazine Attack Killed; Mark...   \n",
      "5248   With only about 70 days left until the electio...   \n",
      "2943   Islamic State militants have acknowledged for ...   \n",
      "356    (CNN) Thick fog forced authorities to suspend ...   \n",
      "2786   (CNN) Aerial bombardments blew apart a Doctors...   \n",
      "496    Nearly 300K New Jobs In February; Unemployment...   \n",
      "3622   A member of Al Qaeda's branch in Yemen said Fr...   \n",
      "4748   Let's pretend for a moment that the biggest he...   \n",
      "4953   A couple of weeks ago in this space I pushed b...   \n",
      "4025   The State Department identified two Americans ...   \n",
      "1602   A new national poll shows Vice President Biden...   \n",
      "3634   The Latest On Paris Attack: Manhunt Continues;...   \n",
      "...                                                  ...   \n",
      "9538                                                FAKE   \n",
      "5961                                                FAKE   \n",
      "9752                                                FAKE   \n",
      "7117                                                FAKE   \n",
      "6161                                                FAKE   \n",
      "2504                                                REAL   \n",
      "9466                                                FAKE   \n",
      "8271                                                FAKE   \n",
      "6596                                                FAKE   \n",
      "8170                                                FAKE   \n",
      "3216                                                REAL   \n",
      "4201                                                REAL   \n",
      "4192                                                REAL   \n",
      "9850                                                FAKE   \n",
      "3598                                                REAL   \n",
      "10244                                               FAKE   \n",
      "5837                                                FAKE   \n",
      "2787                                                REAL   \n",
      "9229                                                FAKE   \n",
      "8556                                                FAKE   \n",
      "2966                                                REAL   \n",
      "1433                                                REAL   \n",
      "328                                                 REAL   \n",
      "4723                                                REAL   \n",
      "7927                                                FAKE   \n",
      "802                                                 REAL   \n",
      "1026                                                REAL   \n",
      "1459                                                REAL   \n",
      "5366                                                FAKE   \n",
      "9673                                                FAKE   \n",
      "\n",
      "                                                      X1    X2  \n",
      "ID                                                              \n",
      "6268                            Stagnation for the 95%    FAKE  \n",
      "10499                                               FAKE   NaN  \n",
      "6717                                                FAKE   NaN  \n",
      "8748                                                FAKE   NaN  \n",
      "5741                                                FAKE   NaN  \n",
      "10138                                               FAKE   NaN  \n",
      "10492                                               FAKE   NaN  \n",
      "6404                                                FAKE   NaN  \n",
      "8470                                                FAKE   NaN  \n",
      "7559                                                FAKE   NaN  \n",
      "9954                                                FAKE   NaN  \n",
      "10194                                               FAKE   NaN  \n",
      "9203                                                FAKE   NaN  \n",
      "9097                                                FAKE   NaN  \n",
      "7375                                                FAKE   NaN  \n",
      "9      PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  \n",
      "3508                                                REAL   NaN  \n",
      "2738                                                REAL   NaN  \n",
      "3624                                                REAL   NaN  \n",
      "5248                                                REAL   NaN  \n",
      "2943                                                REAL   NaN  \n",
      "356                                                 REAL   NaN  \n",
      "2786                                                REAL   NaN  \n",
      "496                                                 REAL   NaN  \n",
      "3622                                                REAL   NaN  \n",
      "4748                                                REAL   NaN  \n",
      "4953                                                REAL   NaN  \n",
      "4025                                                REAL   NaN  \n",
      "1602                                                REAL   NaN  \n",
      "3634                                                REAL   NaN  \n",
      "...                                                  ...   ...  \n",
      "9538                                                 NaN   NaN  \n",
      "5961                                                 NaN   NaN  \n",
      "9752                                                 NaN   NaN  \n",
      "7117                                                 NaN   NaN  \n",
      "6161                                                 NaN   NaN  \n",
      "2504                                                 NaN   NaN  \n",
      "9466                                                 NaN   NaN  \n",
      "8271                                                 NaN   NaN  \n",
      "6596                                                 NaN   NaN  \n",
      "8170                                                 NaN   NaN  \n",
      "3216                                                 NaN   NaN  \n",
      "4201                                                 NaN   NaN  \n",
      "4192                                                 NaN   NaN  \n",
      "9850                                                 NaN   NaN  \n",
      "3598                                                 NaN   NaN  \n",
      "10244                                                NaN   NaN  \n",
      "5837                                                 NaN   NaN  \n",
      "2787                                                 NaN   NaN  \n",
      "9229                                                 NaN   NaN  \n",
      "8556                                                 NaN   NaN  \n",
      "2966                                                 NaN   NaN  \n",
      "1433                                                 NaN   NaN  \n",
      "328                                                  NaN   NaN  \n",
      "4723                                                 NaN   NaN  \n",
      "7927                                                 NaN   NaN  \n",
      "802                                                  NaN   NaN  \n",
      "1026                                                 NaN   NaN  \n",
      "1459                                                 NaN   NaN  \n",
      "5366                                                 NaN   NaN  \n",
      "9673                                                 NaN   NaN  \n",
      "\n",
      "[3999 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.sort_values(by='X1', ascending=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that some texts have been split and moved to the label column, and the X1 column has therefore the FAKE or REAL label. The same happens with the X2 column. Let's then move texts in X1 and X2 columns to the text column, and the FAKE or REAL values to the label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row['X1'] == \"FAKE\" or row['X1'] == \"REAL\":\n",
    "        row['text'] = row['text'] + \" \" + row['label']\n",
    "        row['label'] = row['X1']\n",
    "        row['X1'] = np.nan\n",
    "        \n",
    "for index, row in df.iterrows():\n",
    "    if row['X2'] == \"FAKE\" or row['X2'] == \"REAL\":\n",
    "        row['text'] = row['text'] + \" \" + row['label'] + \" \" + row['X1']\n",
    "        row['label'] = row['X2']\n",
    "        row['X1'] = np.nan\n",
    "        row['X2'] = np.nan\n",
    "        \n",
    "len(df)-sum(pd.isnull(df['X1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)-sum(pd.isnull(df['X2']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we don't have any values in X1 and X2 columns.\n",
    "<br>\n",
    "<br>\n",
    "We'll create the variable 'y' with all the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'y'\n",
    "y = df.label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our opinion, we believe that we should merge 'title' and 'text' columns into one 'all' column, as there might be some informative words in the title that will help to achieve a better accuracy in the later classification.\n",
    "<br>\n",
    "<br>\n",
    "Columns X1 and X2 can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>You Can Smell Hillary’s Fear Daniel Greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>The Battle of New York: Why This Primary Matte...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \\\n",
       "ID                                                               \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL   \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n",
       "875    It's primary day in New York and front-runners...  REAL   \n",
       "\n",
       "                                                     all  \n",
       "ID                                                        \n",
       "8476   You Can Smell Hillary’s Fear Daniel Greenfield...  \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...  \n",
       "3608   Kerry to go to Paris in gesture of sympathy U....  \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...  \n",
       "875    The Battle of New York: Why This Primary Matte...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Paste together title and text into a column \"all\"\n",
    "df[\"all\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
    "df = df.drop(['X1', 'X2'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the training and test sets with the train_test_split function. We take two thirds for training and one third for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make training and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['all'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below we set two classes to define both count and tfidf vectorizers with stemming (using the Snowball Stemmer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "    \n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function 'fake_news' with the following parameters:\n",
    "- ngramrange: this is the range to take for the ngrams\n",
    "- stop: a list of words to be used as stop words\n",
    "- vect: the name of the vectorizer\n",
    "- classifier: the name of the classifier to be used\n",
    "- X_train, X_test, y_train and y_test: vectors with all the sets for training and test news and labels\n",
    "\n",
    "This function will return the accuracy (score) for every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_news(ngramrange, stop, vect, classifier, X_train = X_train, X_test = X_test, y_train = y_train, y_test = y_test):\n",
    "        \n",
    "    if vect == \"count\": \n",
    "        # Initialize the 'count_vectorizer'\n",
    "        count_vectorizer = CountVectorizer(stop_words = stop, ngram_range = ngramrange)\n",
    "        # Fit and transform the training data \n",
    "        train = count_vectorizer.fit_transform(X_train) \n",
    "        # Transform the test set\n",
    "        test = count_vectorizer.transform(X_test)\n",
    "        \n",
    "    elif vect == \"tfidf\":\n",
    "        # Initialize the 'tfidf_vectorizer' \n",
    "        tfidf_vectorizer = TfidfVectorizer(stop_words = stop, max_df = 0.7, ngram_range = ngramrange) \n",
    "        train = tfidf_vectorizer.fit_transform(X_train)  \n",
    "        test = tfidf_vectorizer.transform(X_test)\n",
    "        \n",
    "    elif vect == \"stemcount\":\n",
    "        # Initialize the 'stemmed_count_vectorizer'\n",
    "        stemmed_count_vect = StemmedCountVectorizer(stop_words = stop, ngram_range = ngramrange)\n",
    "        train = stemmed_count_vect.fit_transform(X_train) \n",
    "        test = stemmed_count_vect.transform(X_test)\n",
    "        \n",
    "    elif vect == \"stemtfidf\":\n",
    "        # Initialize the 'stemmed_tfidf_vectorizer'\n",
    "        stemmed_tfidf_vect = StemmedTfidfVectorizer(stop_words = stop, max_df = 0.7, ngram_range = ngramrange)\n",
    "        train = stemmed_tfidf_vect.fit_transform(X_train) \n",
    "        test = stemmed_tfidf_vect.transform(X_test)\n",
    "    \n",
    "    if classifier == \"NB\":\n",
    "        # Instantiate a Multinomial Naive Bayes classifier\n",
    "        clf = MultinomialNB() \n",
    "              \n",
    "    elif classifier == \"PA\":\n",
    "        # Instantiate a Passive-Aggresive classifier\n",
    "        clf = PassiveAggressiveClassifier(max_iter=50, n_jobs=-1)\n",
    "\n",
    "    elif classifier == \"SVC\":\n",
    "        # Instantiate a SVC Classifier\n",
    "        clf = LinearSVC()\n",
    "        \n",
    "    # Fit the classifier to the training data\n",
    "    clf.fit(train, y_train)\n",
    "    # Create the predicted tags\n",
    "    pred = clf.predict(test)    \n",
    "        \n",
    "    # Calculate the accuracy score\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now initialize a table to compare all the possible combinations of ngrams, stopwords, vectorizers and classifiers with their accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names =  ['ACCURACY', 'VECTORIZER', 'CLASSIFIER', 'NGRAMS', 'STOPWORDS']\n",
    "acc_table = pd.DataFrame(columns = col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stopwords we can use the 'english' list, but we will also create another list (stopset) removing from the english list some words that can help to better perform the classification.\n",
    "We also create lists of ngrams, stopwords, vectorizers and classifiers to be used in the 'fake_news' function as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopset = set(stopwords.words('english')) - set(('over', 'under', 'below', 'more', 'most', 'no', 'not', 'only', 'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))\n",
    "ngrams_list = [(1,1), (1,2), (1,3)]\n",
    "stopwords_list = [None, \"english\", stopset]\n",
    "vectorizers_list = [\"count\", \"tfidf\", \"stemcount\", \"stemtfidf\"]\n",
    "classifiers_list = [\"NB\", \"PA\", \"SVC\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to call the 'fake_news' function with all the possible combinations of the above lists. To make that, we can use 4 nested _for_ loops. In every round a new row with the resulting accuracy score is added to the table for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating score 1 of 108\n",
      "Calculating score 2 of 108\n",
      "Calculating score 3 of 108\n",
      "Calculating score 4 of 108\n",
      "Calculating score 5 of 108\n",
      "Calculating score 6 of 108\n",
      "Calculating score 7 of 108\n",
      "Calculating score 8 of 108\n",
      "Calculating score 9 of 108\n",
      "Calculating score 10 of 108\n",
      "Calculating score 11 of 108\n",
      "Calculating score 12 of 108\n",
      "Calculating score 13 of 108\n",
      "Calculating score 14 of 108\n",
      "Calculating score 15 of 108\n",
      "Calculating score 16 of 108\n",
      "Calculating score 17 of 108\n",
      "Calculating score 18 of 108\n",
      "Calculating score 19 of 108\n",
      "Calculating score 20 of 108\n",
      "Calculating score 21 of 108\n",
      "Calculating score 22 of 108\n",
      "Calculating score 23 of 108\n",
      "Calculating score 24 of 108\n",
      "Calculating score 25 of 108\n",
      "Calculating score 26 of 108\n",
      "Calculating score 27 of 108\n",
      "Calculating score 28 of 108\n",
      "Calculating score 29 of 108\n",
      "Calculating score 30 of 108\n",
      "Calculating score 31 of 108\n",
      "Calculating score 32 of 108\n",
      "Calculating score 33 of 108\n",
      "Calculating score 34 of 108\n",
      "Calculating score 35 of 108\n",
      "Calculating score 36 of 108\n",
      "Calculating score 37 of 108\n",
      "Calculating score 38 of 108\n",
      "Calculating score 39 of 108\n",
      "Calculating score 40 of 108\n",
      "Calculating score 41 of 108\n",
      "Calculating score 42 of 108\n",
      "Calculating score 43 of 108\n",
      "Calculating score 44 of 108\n",
      "Calculating score 45 of 108\n",
      "Calculating score 46 of 108\n",
      "Calculating score 47 of 108\n",
      "Calculating score 48 of 108\n",
      "Calculating score 49 of 108\n",
      "Calculating score 50 of 108\n",
      "Calculating score 51 of 108\n",
      "Calculating score 52 of 108\n",
      "Calculating score 53 of 108\n",
      "Calculating score 54 of 108\n",
      "Calculating score 55 of 108\n",
      "Calculating score 56 of 108\n",
      "Calculating score 57 of 108\n",
      "Calculating score 58 of 108\n",
      "Calculating score 59 of 108\n",
      "Calculating score 60 of 108\n",
      "Calculating score 61 of 108\n",
      "Calculating score 62 of 108\n",
      "Calculating score 63 of 108\n",
      "Calculating score 64 of 108\n",
      "Calculating score 65 of 108\n",
      "Calculating score 66 of 108\n",
      "Calculating score 67 of 108\n",
      "Calculating score 68 of 108\n",
      "Calculating score 69 of 108\n",
      "Calculating score 70 of 108\n",
      "Calculating score 71 of 108\n",
      "Calculating score 72 of 108\n",
      "Calculating score 73 of 108\n",
      "Calculating score 74 of 108\n",
      "Calculating score 75 of 108\n",
      "Calculating score 76 of 108\n",
      "Calculating score 77 of 108\n",
      "Calculating score 78 of 108\n",
      "Calculating score 79 of 108\n",
      "Calculating score 80 of 108\n",
      "Calculating score 81 of 108\n",
      "Calculating score 82 of 108\n",
      "Calculating score 83 of 108\n",
      "Calculating score 84 of 108\n",
      "Calculating score 85 of 108\n",
      "Calculating score 86 of 108\n",
      "Calculating score 87 of 108\n",
      "Calculating score 88 of 108\n",
      "Calculating score 89 of 108\n",
      "Calculating score 90 of 108\n",
      "Calculating score 91 of 108\n",
      "Calculating score 92 of 108\n",
      "Calculating score 93 of 108\n",
      "Calculating score 94 of 108\n",
      "Calculating score 95 of 108\n",
      "Calculating score 96 of 108\n",
      "Calculating score 97 of 108\n",
      "Calculating score 98 of 108\n",
      "Calculating score 99 of 108\n",
      "Calculating score 100 of 108\n",
      "Calculating score 101 of 108\n",
      "Calculating score 102 of 108\n",
      "Calculating score 103 of 108\n",
      "Calculating score 104 of 108\n",
      "Calculating score 105 of 108\n",
      "Calculating score 106 of 108\n",
      "Calculating score 107 of 108\n",
      "Calculating score 108 of 108\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total = len(ngrams_list) * len(stopwords_list) * len(vectorizers_list) * len(classifiers_list)\n",
    "\n",
    "for ngramrange in ngrams_list:\n",
    "    for stop in stopwords_list:\n",
    "        for vect in vectorizers_list:\n",
    "            for classifier in classifiers_list:\n",
    "                i = i + 1\n",
    "                print(\"Calculating score\", i, \"of\", total)\n",
    "                score = fake_news(ngramrange, stop, vect, classifier)\n",
    "                acc_table.loc[len(acc_table)] = [score, vect, classifier, ngramrange, stop]\n",
    "\n",
    "acc_table = acc_table.sort_values(by='ACCURACY', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCURACY</th>\n",
       "      <th>VECTORIZER</th>\n",
       "      <th>CLASSIFIER</th>\n",
       "      <th>NGRAMS</th>\n",
       "      <th>STOPWORDS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.942424</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.941667</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.941667</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.940909</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.940152</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.940152</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.939394</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.939394</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.939394</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.939394</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.938636</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.937879</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.937879</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.937121</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.937121</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.937121</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.936364</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.936364</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.936364</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>0.936364</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>0.936364</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.934848</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>0.934848</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.934848</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.934091</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.934091</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>PA</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>0.933333</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>0.932576</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.931061</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>SVC</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.871970</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.871970</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.871970</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.868939</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.868939</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.868182</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.867424</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.863636</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.862879</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.859848</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>0.852273</td>\n",
       "      <td>stemcount</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.850758</td>\n",
       "      <td>count</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.804545</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.797727</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.796212</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.792424</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.789394</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>0.771970</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>0.768939</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.765909</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.765152</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>english</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0.762879</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.758333</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>0.756818</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.752273</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{do, couldn't, haven't, been, ourselves, mustn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>0.717424</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>0.715909</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.712879</td>\n",
       "      <td>tfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.704545</td>\n",
       "      <td>stemtfidf</td>\n",
       "      <td>NB</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ACCURACY VECTORIZER CLASSIFIER  NGRAMS  \\\n",
       "70   0.942424  stemtfidf         PA  (1, 2)   \n",
       "52   0.941667      tfidf         PA  (1, 2)   \n",
       "64   0.941667      tfidf         PA  (1, 2)   \n",
       "23   0.940909  stemtfidf        SVC  (1, 1)   \n",
       "35   0.940152  stemtfidf        SVC  (1, 1)   \n",
       "40   0.940152      tfidf         PA  (1, 2)   \n",
       "76   0.939394      tfidf         PA  (1, 3)   \n",
       "11   0.939394  stemtfidf        SVC  (1, 1)   \n",
       "71   0.939394  stemtfidf        SVC  (1, 2)   \n",
       "106  0.939394  stemtfidf         PA  (1, 3)   \n",
       "28   0.938636      tfidf         PA  (1, 1)   \n",
       "58   0.937879  stemtfidf         PA  (1, 2)   \n",
       "65   0.937879      tfidf        SVC  (1, 2)   \n",
       "46   0.937121  stemtfidf         PA  (1, 2)   \n",
       "29   0.937121      tfidf        SVC  (1, 1)   \n",
       "100  0.937121      tfidf         PA  (1, 3)   \n",
       "5    0.936364      tfidf        SVC  (1, 1)   \n",
       "4    0.936364      tfidf         PA  (1, 1)   \n",
       "17   0.936364      tfidf        SVC  (1, 1)   \n",
       "88   0.936364      tfidf         PA  (1, 3)   \n",
       "82   0.936364  stemtfidf         PA  (1, 3)   \n",
       "53   0.934848      tfidf        SVC  (1, 2)   \n",
       "83   0.934848  stemtfidf        SVC  (1, 3)   \n",
       "10   0.934848  stemtfidf         PA  (1, 1)   \n",
       "59   0.934091  stemtfidf        SVC  (1, 2)   \n",
       "16   0.934091      tfidf         PA  (1, 1)   \n",
       "34   0.933333  stemtfidf         PA  (1, 1)   \n",
       "101  0.933333      tfidf        SVC  (1, 3)   \n",
       "107  0.932576  stemtfidf        SVC  (1, 3)   \n",
       "89   0.931061      tfidf        SVC  (1, 3)   \n",
       "..        ...        ...        ...     ...   \n",
       "48   0.871970      count         NB  (1, 2)   \n",
       "66   0.871970  stemcount         NB  (1, 2)   \n",
       "60   0.871970      count         NB  (1, 2)   \n",
       "54   0.868939  stemcount         NB  (1, 2)   \n",
       "36   0.868939      count         NB  (1, 2)   \n",
       "42   0.868182  stemcount         NB  (1, 2)   \n",
       "84   0.867424      count         NB  (1, 3)   \n",
       "90   0.863636  stemcount         NB  (1, 3)   \n",
       "96   0.862879      count         NB  (1, 3)   \n",
       "102  0.859848  stemcount         NB  (1, 3)   \n",
       "78   0.852273  stemcount         NB  (1, 3)   \n",
       "72   0.850758      count         NB  (1, 3)   \n",
       "21   0.804545  stemtfidf         NB  (1, 1)   \n",
       "33   0.797727  stemtfidf         NB  (1, 1)   \n",
       "15   0.796212      tfidf         NB  (1, 1)   \n",
       "27   0.792424      tfidf         NB  (1, 1)   \n",
       "9    0.789394  stemtfidf         NB  (1, 1)   \n",
       "3    0.772727      tfidf         NB  (1, 1)   \n",
       "87   0.771970      tfidf         NB  (1, 3)   \n",
       "93   0.768939  stemtfidf         NB  (1, 3)   \n",
       "51   0.765909      tfidf         NB  (1, 2)   \n",
       "57   0.765152  stemtfidf         NB  (1, 2)   \n",
       "105  0.762879  stemtfidf         NB  (1, 3)   \n",
       "99   0.758333      tfidf         NB  (1, 3)   \n",
       "69   0.756818  stemtfidf         NB  (1, 2)   \n",
       "63   0.752273      tfidf         NB  (1, 2)   \n",
       "75   0.717424      tfidf         NB  (1, 3)   \n",
       "81   0.715909  stemtfidf         NB  (1, 3)   \n",
       "39   0.712879      tfidf         NB  (1, 2)   \n",
       "45   0.704545  stemtfidf         NB  (1, 2)   \n",
       "\n",
       "                                             STOPWORDS  \n",
       "70   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "52                                             english  \n",
       "64   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "23                                             english  \n",
       "35   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "40                                                None  \n",
       "76                                                None  \n",
       "11                                                None  \n",
       "71   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "106  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "28   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "58                                             english  \n",
       "65   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "46                                                None  \n",
       "29   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "100  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "5                                                 None  \n",
       "4                                                 None  \n",
       "17                                             english  \n",
       "88                                             english  \n",
       "82                                                None  \n",
       "53                                             english  \n",
       "83                                                None  \n",
       "10                                                None  \n",
       "59                                             english  \n",
       "16                                             english  \n",
       "34   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "101  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "107  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "89                                             english  \n",
       "..                                                 ...  \n",
       "48                                             english  \n",
       "66   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "60   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "54                                             english  \n",
       "36                                                None  \n",
       "42                                                None  \n",
       "84                                             english  \n",
       "90                                             english  \n",
       "96   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "102  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "78                                                None  \n",
       "72                                                None  \n",
       "21                                             english  \n",
       "33   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "15                                             english  \n",
       "27   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "9                                                 None  \n",
       "3                                                 None  \n",
       "87                                             english  \n",
       "93                                             english  \n",
       "51                                             english  \n",
       "57                                             english  \n",
       "105  {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "99   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "69   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "63   {do, couldn't, haven't, been, ourselves, mustn...  \n",
       "75                                                None  \n",
       "81                                                None  \n",
       "39                                                None  \n",
       "45                                                None  \n",
       "\n",
       "[108 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the best accuracy (0.9424) score is achieved with the Passive-Aggressive classifier with stemmed Tfidf vectorizer, bigrams and the stopset list of stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also try a SVM pipeline model to check its accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.925\n"
     ]
    }
   ],
   "source": [
    "# Support vector machines without taking stop words out \n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42)),])\n",
    "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Performance\n",
    "pred = text_clf_svm.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred) \n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:   0.919\n"
     ]
    }
   ],
   "source": [
    "# Support vector machines after taking stopwords out\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42)),])\n",
    "text_clf_svm = text_clf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Performance\n",
    "pred = text_clf_svm.predict(X_test)\n",
    "score = metrics.accuracy_score(y_test, pred) \n",
    "print(\"accuracy:   %0.3f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the SVM classifier with and without stopwords are 0.925 and 0.919 (worse than before) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try a Maximum Entropy model to check its performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.615"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxEnt model\n",
    "\n",
    "realnews = []\n",
    "fakenews = []\n",
    "for index, row in df.iterrows():\n",
    "    if row[\"label\"] == \"REAL\":\n",
    "        realnews.append(row[\"all\"])\n",
    "    else:\n",
    "        fakenews.append(row[\"all\"])  \n",
    "\n",
    "def word_split(data):    \n",
    "    data_new = []\n",
    "    for word in data:\n",
    "        word_filter = [i.lower() for i in word.split()]\n",
    "        data_new.append(word_filter)\n",
    "    return data_new\n",
    "\n",
    "def word_feats(words):    \n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "def evaluate_classifier(featx):\n",
    "    \n",
    "    fakefeats = [(featx(f), 'fake') for f in word_split(fakenews)]\n",
    "    realfeats = [(featx(f), 'real') for f in word_split(realnews)]\n",
    "        \n",
    "    fakecutoff = int(len(fakefeats)*3/4)\n",
    "    realcutoff = int(len(realfeats)*3/4)\n",
    " \n",
    "    trainfeats = fakefeats[:fakecutoff] + realfeats[:realcutoff]\n",
    "    testfeats = fakefeats[fakecutoff:] + realfeats[realcutoff:]\n",
    "    \n",
    "    classifier = MaxentClassifier.train(trainfeats, 'GIS', trace=0, encoding=None, labels=None, gaussian_prior_sigma=0, max_iter = 1)\n",
    "    \n",
    "    refsets = collections.defaultdict(set)\n",
    "    testsets = collections.defaultdict(set)\n",
    "    \n",
    "    for i, (feats, label) in enumerate(testfeats):\n",
    "                refsets[label].add(i)\n",
    "                observed = classifier.classify(feats)\n",
    "                testsets[observed].add(i)\n",
    " \n",
    "    accuracy = nltk.classify.util.accuracy(classifier, testfeats)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "evaluate_classifier(word_feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is 0.615 (much worse than any previous method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance until now is Passive-Aggressive with an accuracy of 0.9424."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform now a Grid Search method combined with Cross Validation (3-fold) in order to tune the parameters for our best model.\n",
    "<br>\n",
    "<br>\n",
    "For the Grid Search, we will use a pipeline with the count vectorizer and the tfidf transformer (which is the same as using the tfidf vectorizer). Ideally we would have to use the stemmed version of the count vectorizer, but it takes a lot of time to compute, so we have decided to go for the \"non-stemmed\" version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', PassiveAggressiveClassifier())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 11 parameters in total to add into the Grid Search. The number of combinations with 2 or 3 values per parameter is 116640, which makes it impossible to perform in a short time (it will take aproximmately 162 hours to compute). Therefore, we have decided to perform the Grid Search taking 3 parameters every time, and saving the best values for those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__fit_intercept': (True, False),\n",
      " 'tfidf__norm': ('l1', 'l2', None),\n",
      " 'vect__lowercase': (True, False)}\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   15.2s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   17.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   27.4s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   29.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   30.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   40.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   42.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   42.7s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   53.6s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   55.7s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   57.0s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  30 out of  36 | elapsed:  1.8min remaining:   21.5s\n",
      "[Parallel(n_jobs=-1)]: Done  31 out of  36 | elapsed:  1.8min remaining:   17.3s\n",
      "[Parallel(n_jobs=-1)]: Done  32 out of  36 | elapsed:  1.8min remaining:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 out of  36 | elapsed:  2.0min remaining:   10.6s\n",
      "[Parallel(n_jobs=-1)]: Done  34 out of  36 | elapsed:  2.0min remaining:    7.0s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:  2.0min finished\n",
      "done in 131.170s\n",
      "\n",
      "Best score: 0.925\n",
      "Best parameters set:\n",
      "\tclf__fit_intercept: True\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__lowercase: False\n"
     ]
    }
   ],
   "source": [
    "parameters1 = {\n",
    "    'vect__lowercase': (True, False),\n",
    "    #'vect__stop_words': (stopset, 'english', None),\n",
    "    #'vect__max_features': (None, 1000, 5000, 10000, 50000),\n",
    "    #'vect__strip_accents': ('ascii', 'unicode', None),\n",
    "    'tfidf__norm': ('l1', 'l2', None),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    'clf__fit_intercept': (True, False),\n",
    "    #'clf__max_iter': (10, 100, 1000),\n",
    "    #'clf__random_state': (29, 850, 3866),\n",
    "    #'clf__warm_start': (True, False),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # find the best parameters for both the feature extraction and the\n",
    "    # classifier\n",
    "    grid_search = GridSearchCV(pipeline, parameters1, n_jobs=-1, verbose=50, return_train_score=False)\n",
    "\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters1)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters1 = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters1[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__max_iter': (10, 100, 1000),\n",
      " 'tfidf__use_idf': (True, False),\n",
      " 'vect__stop_words': ({'a',\n",
      "                       'about',\n",
      "                       'above',\n",
      "                       'after',\n",
      "                       'again',\n",
      "                       'against',\n",
      "                       'ain',\n",
      "                       'all',\n",
      "                       'am',\n",
      "                       'an',\n",
      "                       'and',\n",
      "                       'are',\n",
      "                       'aren',\n",
      "                       \"aren't\",\n",
      "                       'as',\n",
      "                       'at',\n",
      "                       'be',\n",
      "                       'because',\n",
      "                       'been',\n",
      "                       'before',\n",
      "                       'being',\n",
      "                       'between',\n",
      "                       'both',\n",
      "                       'but',\n",
      "                       'by',\n",
      "                       'can',\n",
      "                       'couldn',\n",
      "                       \"couldn't\",\n",
      "                       'd',\n",
      "                       'did',\n",
      "                       'didn',\n",
      "                       \"didn't\",\n",
      "                       'do',\n",
      "                       'does',\n",
      "                       'doesn',\n",
      "                       \"doesn't\",\n",
      "                       'doing',\n",
      "                       'don',\n",
      "                       \"don't\",\n",
      "                       'down',\n",
      "                       'during',\n",
      "                       'each',\n",
      "                       'for',\n",
      "                       'from',\n",
      "                       'further',\n",
      "                       'had',\n",
      "                       'hadn',\n",
      "                       \"hadn't\",\n",
      "                       'has',\n",
      "                       'hasn',\n",
      "                       \"hasn't\",\n",
      "                       'have',\n",
      "                       'haven',\n",
      "                       \"haven't\",\n",
      "                       'having',\n",
      "                       'he',\n",
      "                       'her',\n",
      "                       'here',\n",
      "                       'hers',\n",
      "                       'herself',\n",
      "                       'him',\n",
      "                       'himself',\n",
      "                       'his',\n",
      "                       'how',\n",
      "                       'i',\n",
      "                       'if',\n",
      "                       'in',\n",
      "                       'into',\n",
      "                       'is',\n",
      "                       'isn',\n",
      "                       \"isn't\",\n",
      "                       'it',\n",
      "                       \"it's\",\n",
      "                       'its',\n",
      "                       'itself',\n",
      "                       'll',\n",
      "                       'm',\n",
      "                       'ma',\n",
      "                       'me',\n",
      "                       'mightn',\n",
      "                       \"mightn't\",\n",
      "                       'mustn',\n",
      "                       \"mustn't\",\n",
      "                       'my',\n",
      "                       'myself',\n",
      "                       'needn',\n",
      "                       \"needn't\",\n",
      "                       'nor',\n",
      "                       'now',\n",
      "                       'o',\n",
      "                       'of',\n",
      "                       'off',\n",
      "                       'on',\n",
      "                       'or',\n",
      "                       'other',\n",
      "                       'our',\n",
      "                       'ours',\n",
      "                       'ourselves',\n",
      "                       'out',\n",
      "                       'own',\n",
      "                       're',\n",
      "                       's',\n",
      "                       'same',\n",
      "                       'shan',\n",
      "                       \"shan't\",\n",
      "                       'she',\n",
      "                       \"she's\",\n",
      "                       'should',\n",
      "                       \"should've\",\n",
      "                       'shouldn',\n",
      "                       \"shouldn't\",\n",
      "                       'some',\n",
      "                       't',\n",
      "                       'than',\n",
      "                       'that',\n",
      "                       \"that'll\",\n",
      "                       'the',\n",
      "                       'their',\n",
      "                       'theirs',\n",
      "                       'them',\n",
      "                       'themselves',\n",
      "                       'then',\n",
      "                       'there',\n",
      "                       'these',\n",
      "                       'they',\n",
      "                       'this',\n",
      "                       'those',\n",
      "                       'through',\n",
      "                       'to',\n",
      "                       'until',\n",
      "                       'up',\n",
      "                       've',\n",
      "                       'was',\n",
      "                       'wasn',\n",
      "                       \"wasn't\",\n",
      "                       'we',\n",
      "                       'were',\n",
      "                       'weren',\n",
      "                       \"weren't\",\n",
      "                       'what',\n",
      "                       'when',\n",
      "                       'where',\n",
      "                       'which',\n",
      "                       'while',\n",
      "                       'who',\n",
      "                       'whom',\n",
      "                       'why',\n",
      "                       'will',\n",
      "                       'with',\n",
      "                       'won',\n",
      "                       \"won't\",\n",
      "                       'wouldn',\n",
      "                       \"wouldn't\",\n",
      "                       'y',\n",
      "                       'you',\n",
      "                       \"you'd\",\n",
      "                       \"you'll\",\n",
      "                       \"you're\",\n",
      "                       \"you've\",\n",
      "                       'your',\n",
      "                       'yours',\n",
      "                       'yourself',\n",
      "                       'yourselves'},\n",
      "                      'english',\n",
      "                      None)}\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   11.0s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   13.3s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   13.5s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   21.7s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   23.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   27.2s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   33.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   35.2s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   37.9s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   38.1s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   43.7s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   45.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   47.5s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   56.8s\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:   59.4s\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  3.5min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  54 | elapsed:  3.9min remaining:   23.7s\n",
      "[Parallel(n_jobs=-1)]: Done  51 out of  54 | elapsed:  4.1min remaining:   14.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:  4.4min finished\n",
      "done in 277.770s\n",
      "\n",
      "Best score: 0.923\n",
      "Best parameters set:\n",
      "\tclf__max_iter: 100\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__stop_words: None\n"
     ]
    }
   ],
   "source": [
    "stopset = set(stopwords.words('english')) - set(('over', 'under', 'below', 'more', 'most', 'no', 'not', 'only', 'such', 'few', 'so', 'too', 'very', 'just', 'any', 'once'))\n",
    "\n",
    "parameters2 = {\n",
    "    #'vect__lowercase': (True, False),\n",
    "    'vect__stop_words': (stopset, 'english', None),\n",
    "    #'vect__max_features': (None, 1000, 5000, 10000, 50000),\n",
    "    #'vect__strip_accents': ('ascii', 'unicode', None),\n",
    "    #'tfidf__norm': ('l1', 'l2', None),\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    #'clf__fit_intercept': (True, False),\n",
    "    'clf__max_iter': (10, 100, 1000),\n",
    "    #'clf__random_state': (29, 850, 3866),\n",
    "    #'clf__warm_start': (True, False),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters2, n_jobs=-1, verbose=50, return_train_score=False)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters2)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters2 = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters2.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters2[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3rd round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__random_state': (29, 850, 3866),\n",
      " 'tfidf__sublinear_tf': (True, False),\n",
      " 'vect__max_features': (None, 1000, 5000, 10000, 50000)}\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   14.8s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   15.6s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   24.3s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   25.4s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   25.7s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   26.7s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   36.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   37.3s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   37.6s\n",
      "[Parallel(n_jobs=-1)]: Done  12 tasks      | elapsed:   38.4s\n",
      "[Parallel(n_jobs=-1)]: Done  13 tasks      | elapsed:   47.4s\n",
      "[Parallel(n_jobs=-1)]: Done  14 tasks      | elapsed:   48.8s\n",
      "[Parallel(n_jobs=-1)]: Done  15 tasks      | elapsed:   49.1s\n",
      "[Parallel(n_jobs=-1)]: Done  16 tasks      | elapsed:   50.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   60.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  19 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  20 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-1)]: Done  21 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  22 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  23 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  27 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  30 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  31 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  32 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  35 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  36 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  38 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  39 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  40 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done  41 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  43 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  44 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  45 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done  49 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  50 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  52 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done  54 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  55 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  56 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  57 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=-1)]: Done  58 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  59 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  61 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  62 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  63 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  65 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done  66 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  67 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  68 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done  69 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  70 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  71 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done  73 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done  74 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  75 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  78 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  79 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  80 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  81 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  82 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  83 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done  85 out of  90 | elapsed:  4.2min remaining:   14.6s\n",
      "[Parallel(n_jobs=-1)]: Done  87 out of  90 | elapsed:  4.2min remaining:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  4.4min finished\n",
      "done in 270.404s\n",
      "\n",
      "Best score: 0.934\n",
      "Best parameters set:\n",
      "\tclf__random_state: 3866\n",
      "\ttfidf__sublinear_tf: True\n",
      "\tvect__max_features: 50000\n"
     ]
    }
   ],
   "source": [
    "parameters3 = {\n",
    "    #'vect__lowercase': (True, False),\n",
    "    #'vect__stop_words': (stopset, 'english', None),\n",
    "    'vect__max_features': (None, 1000, 5000, 10000, 50000),\n",
    "    #'vect__strip_accents': ('ascii', 'unicode', None),\n",
    "    #'tfidf__norm': ('l1', 'l2', None),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    'tfidf__sublinear_tf': (True, False),\n",
    "    #'clf__fit_intercept': (True, False),\n",
    "    #'clf__max_iter': (10, 100, 1000),\n",
    "    'clf__random_state': (29, 850, 3866),\n",
    "    #'clf__warm_start': (True, False),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters3, n_jobs=-1, verbose=50, return_train_score=False)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters3)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters3 = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters3.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters3[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4th round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__warm_start': (True, False),\n",
      " 'vect__strip_accents': ('ascii', 'unicode', None)}\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done   2 tasks      | elapsed:   14.9s\n",
      "[Parallel(n_jobs=-1)]: Done   3 tasks      | elapsed:   15.9s\n",
      "[Parallel(n_jobs=-1)]: Done   4 tasks      | elapsed:   18.7s\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done   6 tasks      | elapsed:   29.0s\n",
      "[Parallel(n_jobs=-1)]: Done   7 tasks      | elapsed:   30.1s\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:   31.4s\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   41.4s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   41.7s\n",
      "[Parallel(n_jobs=-1)]: Done  11 tasks      | elapsed:   42.8s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  18 | elapsed:   44.1s remaining:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done  13 out of  18 | elapsed:   56.3s remaining:   21.6s\n",
      "[Parallel(n_jobs=-1)]: Done  14 out of  18 | elapsed:   56.6s remaining:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  18 | elapsed:   56.7s remaining:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  16 out of  18 | elapsed:   57.6s remaining:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed:  1.1min finished\n",
      "done in 74.915s\n",
      "\n",
      "Best score: 0.924\n",
      "Best parameters set:\n",
      "\tclf__warm_start: False\n",
      "\tvect__strip_accents: None\n"
     ]
    }
   ],
   "source": [
    "parameters4 = {\n",
    "    #'vect__lowercase': (True, False),\n",
    "    #'vect__stop_words': (stopset, 'english', None),\n",
    "    #'vect__max_features': (None, 1000, 5000, 10000, 50000),\n",
    "    'vect__strip_accents': ('ascii', 'unicode', None),\n",
    "    #'tfidf__norm': ('l1', 'l2', None),\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    #'tfidf__sublinear_tf': (True, False),\n",
    "    #'clf__fit_intercept': (True, False),\n",
    "    #'clf__max_iter': (10, 100, 1000),\n",
    "    #'clf__random_state': (29, 850, 3866),\n",
    "    'clf__warm_start': (True, False),\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    grid_search = GridSearchCV(pipeline, parameters4, n_jobs=-1, verbose=50, return_train_score=False)\n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters4)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters4 = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters4.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters4[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print the values for all the parameters to use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tclf__fit_intercept: True\n",
      "\ttfidf__norm: 'l2'\n",
      "\tvect__lowercase: False\n",
      "\tclf__max_iter: 100\n",
      "\ttfidf__use_idf: True\n",
      "\tvect__stop_words: None\n",
      "\tclf__random_state: 3866\n",
      "\ttfidf__sublinear_tf: True\n",
      "\tvect__max_features: 50000\n",
      "\tclf__warm_start: False\n",
      "\tvect__strip_accents: None\n"
     ]
    }
   ],
   "source": [
    "for param_name in sorted(parameters1.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters1[param_name]))\n",
    "for param_name in sorted(parameters2.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters2[param_name]))\n",
    "for param_name in sorted(parameters3.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters3[param_name]))\n",
    "for param_name in sorted(parameters4.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters4[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use now the stemmed version of the tfidf vectorizer, as we will run it only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9606060606060606"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_tfidf_vect = StemmedTfidfVectorizer(stop_words = None, max_df = 0.5, min_df=0, ngram_range = (1,2), strip_accents=None, lowercase=False, max_features=50000, norm='l2', use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
    "train = stemmed_tfidf_vect.fit_transform(X_train) \n",
    "test = stemmed_tfidf_vect.transform(X_test)\n",
    "clf = PassiveAggressiveClassifier(n_jobs=-1, C=1, fit_intercept=True, max_iter=100, tol=0.001, loss='hinge', warm_start=True, average=True, random_state=3866)\n",
    "clf.fit(train, y_train)\n",
    "pred = clf.predict(test)    \n",
    "score = metrics.accuracy_score(y_test, pred)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final accuracy score is **0.9606** (96.06%)\n",
    "<br>\n",
    "<br>\n",
    "Now we train the model with the full training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassiveAggressiveClassifier(C=1, average=True, class_weight=None,\n",
       "              fit_intercept=True, loss='hinge', max_iter=100, n_iter=None,\n",
       "              n_jobs=-1, random_state=3866, shuffle=True, tol=0.001,\n",
       "              verbose=0, warm_start=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train = X_train.append(X_test)\n",
    "full_y_train = y_train.append(y_test)\n",
    "train = stemmed_tfidf_vect.fit_transform(full_train)\n",
    "clf.fit(train, full_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the model to the test file to predict the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2321, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"fake_or_real_news_test.csv\")\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text  \n",
       "0  September New Homes Sales Rise Back To 1992 Le...  \n",
       "1  But when Congress debated and passed the Patie...  \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...  \n",
       "3  Police searching for the second of two escaped...  \n",
       "4  No matter who wins California's 475 delegates ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>all</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...   \n",
       "2439   But when Congress debated and passed the Patie...   \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "4128   Police searching for the second of two escaped...   \n",
       "662    No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                                     all  \n",
       "ID                                                        \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...  \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...  \n",
       "864    Sanders, Cruz resist pressure after NY losses,...  \n",
       "4128   Surviving escaped prisoner likely fatigued and...  \n",
       "662    Clinton and Sanders neck and neck in Californi...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = df2.set_index(\"ID\")\n",
    "df2[\"all\"] = df2[\"title\"] + \" \" + df2[\"text\"]\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKE', 'REAL', 'REAL', ..., 'FAKE', 'REAL', 'REAL'], dtype='<U4')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_news = df2['all']\n",
    "test2 = stemmed_tfidf_vect.transform(df2_news)\n",
    "pred = clf.predict(test2) \n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>all</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...   \n",
       "2439   But when Congress debated and passed the Patie...   \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...   \n",
       "4128   Police searching for the second of two escaped...   \n",
       "662    No matter who wins California's 475 delegates ...   \n",
       "\n",
       "                                                     all label  \n",
       "ID                                                              \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...  FAKE  \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...  REAL  \n",
       "864    Sanders, Cruz resist pressure after NY losses,...  REAL  \n",
       "4128   Surviving escaped prisoner likely fatigued and...  REAL  \n",
       "662    Clinton and Sanders neck and neck in Californi...  REAL  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.insert(loc=3, column='label', value=pred)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label\n",
       "ID         \n",
       "10498  FAKE\n",
       "2439   REAL\n",
       "864    REAL\n",
       "4128   REAL\n",
       "662    REAL"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.to_csv('check.csv', index=True)\n",
    "df2 = df2.drop(['title', 'text', 'all'], axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our text classification model (with a **0.9606 accuracy score**) we carried out the following steps: \n",
    "1. We cleaned the dataset (since some text was splitted and had moved columns)\n",
    "<br>\n",
    "2. We created our training and test set in a 67-33 proportion. \n",
    "<br>\n",
    "3. We compared all the possible combinations of ngrams, stopwords, vectorizers ( both count and tfidf vectorizers with and without stemming, using the Snowball Stemmer) and classifiers (Naive Bayes, SVC, Passive-Agressive) with their accuracies. \n",
    "<br>\n",
    "<br>\n",
    "   As stopwords we used the 'english' list, but also create another list (stopset) removing from the english list some words that helped to better perform the classification.\n",
    "<br>\n",
    "<br>\n",
    "   The best possible result was **Passive-Aggressive classifier with stemmed Tfidf vectorizer, bigrams and the stopset list of stopwords**.\n",
    "<br>\n",
    "<br>\n",
    "4. We also tried SVM and Maximum Entropy, but the accuracy score was worse than before.\n",
    "<br>\n",
    "5. Finally, we applied a Grid Search method combined with Cross Validation (3-fold) in order to tune the parameters for our best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
